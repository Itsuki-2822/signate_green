{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b464b22f-300e-46ca-ace5-5483b3fdb68c",
   "metadata": {},
   "source": [
    "# LB 0.3560778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f81a940-74b8-4c2a-9913-c4746b1090fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/a6/11/5171f6a1ecf7f008648fef6ef780d92414763ff5ba50a796657b9275dc1e/lightgbm-4.2.0-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached lightgbm-4.2.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting category_encoders (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for category_encoders from https://files.pythonhosted.org/packages/7f/e5/79a62e5c9c9ddbfa9ff5222240d408c1eeea4e38741a0dc8343edc7ef1ec/category_encoders-2.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting xgboost (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/c3/eb/496aa2f5d356af4185f770bc76055307f8d1870e11016b10fd779b21769c/xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting geopy (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for geopy from https://files.pythonhosted.org/packages/e5/15/cf2a69ade4b194aa524ac75112d5caac37414b20a3a03e6865dfe0bd1539/geopy-2.4.1-py3-none-any.whl.metadata\n",
      "  Using cached geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting imblearn (from -r requirements.txt (line 5))\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting optuna (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4c/6a/219a431aaf81b3eb3070fd2d58116baa366d3072f43bbcc87dc3495b7546/optuna-3.5.0-py3-none-any.whl.metadata\n",
      "  Using cached optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 2)) (2.0.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 2)) (0.5.3)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy->-r requirements.txt (line 4))\n",
      "  Using cached geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Collecting imbalanced-learn (from imblearn->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Using cached imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/7f/50/9fb3a5c80df6eb6516693270621676980acd6d5a9a7efdbfa273f8d616c7/alembic-1.13.1-py3-none-any.whl.metadata\n",
      "  Using cached alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/95/df/520663eb7f7a329f7c585834b754bcc3cbcc03957d85fcbba4a2a723ad9d/colorlog-6.8.0-py3-none-any.whl.metadata\n",
      "  Using cached colorlog-6.8.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r requirements.txt (line 6)) (2.0.21)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r requirements.txt (line 6)) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna->-r requirements.txt (line 6)) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/24/3b/11fe92d68c6a42468ddab0cf03f454419b0788fff4e91ba46b8bebafeffd/Mako-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached Mako-1.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->-r requirements.txt (line 6)) (2.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna->-r requirements.txt (line 6)) (2.1.3)\n",
      "Using cached lightgbm-4.2.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Using cached category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "Using cached geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "Using cached optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "Using cached colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Using cached imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Using cached Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, geographiclib, colorlog, xgboost, lightgbm, geopy, alembic, optuna, imbalanced-learn, imblearn, category_encoders\n",
      "Successfully installed Mako-1.3.0 alembic-1.13.1 category_encoders-2.6.3 colorlog-6.8.0 geographiclib-2.0 geopy-2.4.1 imbalanced-learn-0.11.0 imblearn-0.0 lightgbm-4.2.0 optuna-3.5.0 xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e77e2ff-03b5-4798-bb56-534b74b7459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80235bf-a8e7-45be-a79e-70bc1b0e26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "Id = pd.DataFrame(test['Unnamed: 0'])\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "test.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46e2a992-49fb-4706-842e-3d02c038c27f",
   "metadata": {},
   "source": [
    "stats = df.groupby('boro_ct')['tree_dbh'].agg(['mean', 'std', 'max', 'min', pd.Series.skew, pd.Series.kurt])\n",
    "\n",
    "# 新しいカスタムの列名を指定\n",
    "stats.columns = ['tree_dbh_mean', 'tree_dbh_std', 'tree_dbh_max', 'tree_dbh_min', 'tree_dbh_skew', 'tree_dbh_kurt']\n",
    "\n",
    "# 結果の統計特徴量を元のデータフレームに結合\n",
    "df = df.merge(stats, on='boro_ct', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faaca6a2-40a3-4854-a5cb-5e34c45def00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at    object\n",
       "tree_dbh       int64\n",
       "curb_loc      object\n",
       "health         int64\n",
       "steward       object\n",
       "guards        object\n",
       "sidewalk      object\n",
       "user_type     object\n",
       "problems      object\n",
       "spc_common    object\n",
       "spc_latin     object\n",
       "nta           object\n",
       "nta_name      object\n",
       "borocode       int64\n",
       "boro_ct        int64\n",
       "boroname      object\n",
       "zip_city      object\n",
       "cb_num         int64\n",
       "st_senate      int64\n",
       "st_assem       int64\n",
       "cncldist       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc810f36-cdb5-41c1-bf5f-a31d69338e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nta'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a1a686-a453-4c3c-a06d-9568660660b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nta_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47124b3d-ca10-43af-8ec3-b59dc36417fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "health\n",
       "1    15751\n",
       "0     3535\n",
       "2      698\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['health'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4831544-dc95-4a3a-a2ed-137208897335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>curb_loc</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steward</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guards</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sidewalk</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_type</td>\n",
       "      <td>2983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nta</td>\n",
       "      <td>8213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>freq_problems</td>\n",
       "      <td>4441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>freq_spc_common</td>\n",
       "      <td>4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>label_spc_latin</td>\n",
       "      <td>4461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>label_nta_name</td>\n",
       "      <td>4214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>label_boroname</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>freq_zip_city</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  importance\n",
       "2          curb_loc         543\n",
       "3           steward         939\n",
       "4            guards        1149\n",
       "5          sidewalk        1453\n",
       "6         user_type        2983\n",
       "10              nta        8213\n",
       "31    freq_problems        4441\n",
       "33  freq_spc_common        4565\n",
       "34  label_spc_latin        4461\n",
       "38   label_nta_name        4214\n",
       "40   label_boroname           4\n",
       "43    freq_zip_city        1242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.copy()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "target_encoder = ce.TargetEncoder()\n",
    "label_encoder = LabelEncoder()\n",
    "binary_encoder = ce.BinaryEncoder()\n",
    "Helmert_encoder = ce.HelmertEncoder()\n",
    "JamesStein_encoder = ce.JamesSteinEncoder()\n",
    "Backward_encoder = ce.BackwardDifferenceEncoder()\n",
    "Polynomial_encoder = ce.PolynomialEncoder()\n",
    "LeaveOneOut_encoder = ce.LeaveOneOutEncoder()\n",
    "WOE_encoder = ce.WOEEncoder()\n",
    "CatBoost_encoder = ce.CatBoostEncoder()\n",
    "\n",
    "\n",
    "def apply_one_hot_encoding(df, column):\n",
    "    try:\n",
    "        onehot_encoded = onehot_encoder.fit_transform(df[[column]])\n",
    "        for i in range(onehot_encoded.shape[1]):\n",
    "            df[f'onehot_{column}_{i}'] = onehot_encoded[:, i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in one-hot encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_label_encoding(df, column):\n",
    "    try:\n",
    "        df[f'label_{column}'] = label_encoder.fit_transform(df[column])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in label encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_frequency_encoding(df, column):\n",
    "    try:\n",
    "        frequency_encoding = df[column].value_counts(normalize=True)\n",
    "        df[f'freq_{column}'] = df[column].map(frequency_encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in frequency encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_binary_encoding(df, column):\n",
    "    try:\n",
    "        binary_encoded = binary_encoder.fit_transform(df[[column]])\n",
    "        for i in range(binary_encoded.shape[1]):\n",
    "            df[f'binary_{column}_{i}'] = binary_encoded[:, i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in binary encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_helmert_encoding(df, column):\n",
    "    try:\n",
    "        # Assuming Helmert_encoder is previously defined and imported\n",
    "        df[f'Helmert_{column}'] = Helmert_encoder.fit_transform(df[[column]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Helmert encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_backward_encoding(df, column):\n",
    "    try:\n",
    "        # Assuming Backward_encoder is previously defined and imported\n",
    "        df[f'Backward_{column}'] = Backward_encoder.fit_transform(df[[column]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in backward difference encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def apply_polynomial_encoding(df, column):\n",
    "    try:\n",
    "        # Assuming Polynomial_encoder is previously defined and imported\n",
    "        df[f'Polynomial_{column}'] = Polynomial_encoder.fit_transform(df[[column]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in polynomial encoding for {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_encodings(df,columns):\n",
    "    for column in columns:\n",
    "        #df = apply_one_hot_encoding(df, column)\n",
    "        df = apply_label_encoding(df, column)\n",
    "        df = apply_frequency_encoding(df, column)\n",
    "        #df = apply_binary_encoding(df, column)\n",
    "        #df = apply_helmert_encoding(df, column)\n",
    "        #df = apply_backward_encoding(df, column)\n",
    "        #df = apply_polynomial_encoding(df, column)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def category(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def choose_encoding(df):\n",
    "    y = df['health']\n",
    "    X = df.drop('health',axis=1)\n",
    "    X=category(X)\n",
    "    #columns = ['curb_loc','steward','guards','sidewalk','spc_latin','nta_name','user_type','problems','boroname']\n",
    "    columns = ['curb_loc','steward','guards','sidewalk','user_type','problems','spc_common','spc_latin','nta','nta_name','boroname','zip_city']\n",
    "    X = apply_encodings(X,columns)\n",
    "    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33,random_state=28)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        #'random_state': 10, \n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1,\n",
    "        #'early_stopping_round': 100,\n",
    "        'feature_importances':'gain'\n",
    "        }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    feature = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_})\n",
    "    feature.sort_values(by='importance',ascending=False)\n",
    "    \n",
    "    df_dict = {}\n",
    "    top_elements = {}\n",
    "\n",
    "    for col in columns:\n",
    "        matched_rows = feature[feature['feature'].str.contains(col)]\n",
    "        df_dict[col] = matched_rows\n",
    "        \n",
    "    for col in df_dict:\n",
    "        sorted_df = df_dict[col].sort_values(by='importance', ascending=False)\n",
    "\n",
    "        if not sorted_df.empty:\n",
    "            top_elements[col] = sorted_df.iloc[0].to_dict()\n",
    "\n",
    "    top_features_list = [value['feature'] for value in top_elements.values()]\n",
    "    selected_features = feature[feature['feature'].isin(top_features_list)]\n",
    "    return selected_features\n",
    "    del y,X,df\n",
    "\n",
    "\n",
    "a = choose_encoding(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77efcf9d-05dc-4771-abdf-56729ef36512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def date(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['created_year'] = df['created_at'].dt.year\n",
    "    df['created_month'] = df['created_at'].dt.month\n",
    "    df['created_date'] = df['created_at'].dt.day\n",
    "    df['created_weekdat'] = df['created_at'].dt.weekday\n",
    "    df.drop('created_at',axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def encoding(df):\n",
    "    label_columns = ['curb_loc','steward','guards','sidewalk','boroname','user_type']\n",
    "    for col in label_columns:\n",
    "        df = apply_label_encoding(df, col)\n",
    "    df.drop(label_columns,axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_geo(df):\n",
    "    locations = [\n",
    "        {\"name\": \"Little Neck, Queens\", \"latitude\": 40.7620, \"longitude\": -73.73821},\n",
    "        {\"name\": \"Bronx, Bronx\", \"latitude\": 40.837048, \"longitude\": -73.8654332},\n",
    "        {\"name\": \"Staten Island, Staten Island\", \"latitude\": 40.579021, \"longitude\": -74.1515353},\n",
    "        {\"name\": \"New York, Manhattan\", \"latitude\": 40.7831, \"longitude\": -73.97124},\n",
    "        {\"name\": \"Flushing, Queens\", \"latitude\": 40.7658, \"longitude\": -73.83315},\n",
    "        {\"name\": \"Saint Albans, Queens\", \"latitude\": 40.6948, \"longitude\": -73.76696},\n",
    "        {\"name\": \"Brooklyn, Brooklyn\", \"latitude\": 40.650002, \"longitude\": -73.9499977},\n",
    "        {\"name\": \"Ozone Park, Queens\", \"latitude\": 40.681938, \"longitude\": -73.8461468},\n",
    "        {\"name\": \"Whitestone, Queens\", \"latitude\": 40.792045, \"longitude\": -73.8095579},\n",
    "        {\"name\": \"Jamaica, Queens\", \"latitude\": 40.699783, \"longitude\": -73.78602610},\n",
    "        {\"name\": \"Rosedale, Queens\", \"latitude\": 40.6584068, \"longitude\": -73.73895961},\n",
    "        {\"name\": \"Oakland Gardens, Queens\", \"latitude\": 40.7408584, \"longitude\": -73.7582412},\n",
    "        {\"name\": \"Elmhurst, Queens\", \"latitude\": 40.7429, \"longitude\": -73.88003},\n",
    "        {\"name\": \"Bellerose, Queens\", \"latitude\": 40.7268, \"longitude\": -73.74154},\n",
    "        {\"name\": \"Forest Hills, Queens\", \"latitude\": 40.719517, \"longitude\": -73.8522115},\n",
    "        {\"name\": \"Middle Village, Queens\", \"latitude\": 40.7184, \"longitude\": -73.882296},\n",
    "        {\"name\": \"Queens Village, Queens\", \"latitude\": 40.7156628, \"longitude\": -73.74190177},\n",
    "        {\"name\": \"Ridgewood, Queens\", \"latitude\": 40.710850, \"longitude\": -73.897766},\n",
    "        {\"name\": \"Jackson Heights, Queens\", \"latitude\": 40.755684, \"longitude\": -73.883072},\n",
    "        {\"name\": \"East Elmhurst, Queens\", \"latitude\": 40.7737505, \"longitude\": -73.871309},\n",
    "        {\"name\": \"Woodside, Queens\", \"latitude\": 40.745, \"longitude\": -73.9052},\n",
    "        {\"name\": \"Astoria, Queens\", \"latitude\": 40.7710, \"longitude\": -73.90473},\n",
    "        {\"name\": \"Long Island City, Queens\", \"latitude\": 40.74553, \"longitude\": -73.94854},\n",
    "        {\"name\": \"Cambria Heights, Queens\", \"latitude\": 40.692158, \"longitude\": -73.7330755},\n",
    "        {\"name\": \"Rockaway Park, Queens\", \"latitude\": 40.5786, \"longitude\": -73.84106},\n",
    "        {\"name\": \"Springfield Gardens, Queens\", \"latitude\": 40.678159, \"longitude\": -73.7465217},\n",
    "        {\"name\": \"Central Park, Manhattan\", \"latitude\": 40.785091, \"longitude\": -73.9682851},\n",
    "        {\"name\": \"Maspeth, Queens\", \"latitude\": 40.7294018, \"longitude\": -73.90658832},\n",
    "        {\"name\": \"Howard Beach, Queens\", \"latitude\": 40.658, \"longitude\": -73.840},\n",
    "        {\"name\": \"South Ozone Park, Queens\", \"latitude\": 40.676, \"longitude\": -73.812},\n",
    "        {\"name\": \"Glen Oaks, Queens\", \"latitude\": 40.7471504, \"longitude\": -73.71182235},\n",
    "        {\"name\": \"Bayside, Queens\", \"latitude\": 40.758556, \"longitude\": -73.7654346},\n",
    "        {\"name\": \"Fresh Meadows, Queens\", \"latitude\": 40.732689, \"longitude\": -73.7848667},\n",
    "        {\"name\": \"Sunnyside, Queens\", \"latitude\": 40.743, \"longitude\": -73.9208},\n",
    "        {\"name\": \"Far Rockaway, Queens\", \"latitude\": 40.6009, \"longitude\": -73.75701},\n",
    "        {\"name\": \"Hollis, Queens\", \"latitude\": 40.711220, \"longitude\": -73.7624952},\n",
    "        {\"name\": \"Kew Gardens, Queens\", \"latitude\": 40.705, \"longitude\": -73.8253},\n",
    "        {\"name\": \"Woodhaven, Queens\", \"latitude\": 40.6901366, \"longitude\": -73.85660875},\n",
    "        {\"name\": \"Rego Park, Queens\", \"latitude\": 40.7237, \"longitude\": -73.70496},\n",
    "        {\"name\": \"Floral Park, Queens\", \"latitude\": 40.7239, \"longitude\": -73.70588},\n",
    "        {\"name\": \"Corona, Queens\", \"latitude\": 40.735, \"longitude\": -73.865},\n",
    "        {\"name\": \"Richmond Hill, Queens\", \"latitude\": 40.695, \"longitude\": -73.83},\n",
    "        {\"name\": \"Arverne, Queens\", \"latitude\": 40.59, \"longitude\": -73.795},\n",
    "        {\"name\": \"College Point, Queens\", \"latitude\": 40.785, \"longitude\": -73.835},\n",
    "        {\"name\": \"Richmond Hill, Queens (alternate)\", \"latitude\": 40.6868, \"longitude\": -73.823},\n",
    "        {\"name\": \"South Richmond Hill, Queens\", \"latitude\": 40.688364, \"longitude\": -73.822763}\n",
    "    ]\n",
    "    locations_1 = pd.DataFrame(locations)\n",
    "    locations_1['combined_address']=locations_1['name']\n",
    "    locations_1.drop('name',axis=1,inplace=True)\n",
    "\n",
    "    locations_1 = locations_1.set_index('combined_address')\n",
    "    \n",
    "    df['combined_address'] = df['zip_city'] + ', ' + df['boroname']\n",
    "    df['latitude'] = df['combined_address'].map(locations_1['latitude'])\n",
    "    df['longitude'] = df['combined_address'].map(locations_1['longitude'])\n",
    "    \n",
    "    df.drop('combined_address',axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def tree(df):\n",
    "    tree = KDTree(df[['latitude', 'longitude']])\n",
    "    distances, _ = tree.query(df[['latitude', 'longitude']], k=2)\n",
    "\n",
    "    nearest_distances = distances[:, 1]\n",
    "    df['nearest_neighbor_distance'] = nearest_distances\n",
    "    return df\n",
    "\n",
    "def clustering(df):\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    df['cluster'] = kmeans.fit_predict(df[['latitude', 'longitude']])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3961cfaa-0f83-4196-9c76-2088cd61d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def minmax(df):\n",
    "    ms = MinMaxScaler()\n",
    "    df['tree_dbh'] = ms.fit_transform(df['tree_dbh'].values.reshape(-1, 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b070dd-4d7e-49ef-a626-800a572e6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_enc(col):\n",
    "    #skf = StratifiedKFold(n_splits=5)\n",
    "    kf = KFold(n_splits=5)\n",
    "    encoded_features = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X,y):\n",
    "        X_train_, X_valid_ = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_ = y.iloc[train_idx]\n",
    "\n",
    "        target_encoder = ce.TargetEncoder()\n",
    "        target_encoder.fit(X_train_[col], y_train_)\n",
    "\n",
    "        X_valid_[f'target_{col}'] = target_encoder.transform(X_valid_[col])\n",
    "        encoded_features.append(X_valid_)\n",
    "\n",
    "\n",
    "    encoded_df = pd.concat(encoded_features).sort_index()\n",
    "    df_with_encoded = pd.merge(X, encoded_df[[f'target_{col}']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    target_encoder = ce.TargetEncoder()\n",
    "    target_encoder.fit(df_with_encoded[col], y)\n",
    "\n",
    "    test[f'target_{col}'] = target_encoder.transform(test[col])\n",
    "\n",
    "    df_with_encoded.drop(col,axis=1,inplace=True)\n",
    "    test.drop(col,axis=1,inplace=True)\n",
    "    \n",
    "    return df_with_encoded, test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "776319f0-54ca-4a77-9e10-9e8c9bdcdbc4",
   "metadata": {},
   "source": [
    "def statistical_features(df):\n",
    "    statistical_columns = ['boro_ct']\n",
    "    for col in statistical_columns:\n",
    "        stats = df.groupby(col)['tree_dbh'].agg(['mean', 'std', 'max', 'min', pd.Series.skew, pd.Series.kurt])\n",
    "        stats.columns = [f'{col}_tree_dbh_mean', f'{col}_tree_dbh_std', f'{col}_tree_dbh_max', f'{col}_tree_dbh_min',f'{col}_tree_dbh_skew',f'{col}_tree_dbh_kurt']\n",
    "        df = df.merge(stats, on=col, how='left')\n",
    "    return df\n",
    "df = statistical_features(df)\n",
    "test = statistical_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab53be8-8fc5-44c8-b861-bee9584ac35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(df):\n",
    "    df.fillna('NULL',inplace=True)\n",
    "    df = df[~df['spc_common'].isin(['Chinese chestnut', 'Himalayan cedar'])]\n",
    "    df = df[~df['nta'].isin(['BK27', 'MN21', 'MN20'])]\n",
    "    df = df[~df['boro_ct'].isin([3046400, 1011201, 1011203, 4112900, 3050500, 4138502, 3080200, 2005002, 4121100, 4018700, 4133900, 3011600, 3028501, 1031703, 3032100, 3044900, 2037800, 3074600, 3082800, 3091000, 4012602, 2007100, 3043900, 3035200, 3022400, 2028100, 4050500, 1008200, 3005000, 4003400, 4045900, 5018701, 3064400, 2002000, 3051602, 3077200, 4041300, 4079702, 3051100, 2039902, 2005600, 1024100, 4057700, 3025000, 3076200, 3050600, 1002602, 2026602, 2039400, 4018800, 2022000, 2034800, 3020400, 3028600, 3118201, 3007100, 2008700, 2025600, 4048000, 3028100, 3019400, 3102200, 4085900, 1013900, 4021900, 3078800, 4013200, 3091600, 3002000, 4004500, 2007700, 2028700, 1004700, 3082400, 3005601, 1012900, 3056300, 4080301, 2006700, 3098800, 3022000, 4024500, 1029300, 2040502, 2014900, 3038900, 4071100, 3021000, 4061900, 4002000, 4014800, 1006800, 4027600, 2039800, 3037401, 3114201, 3028700, 1010400, 3007200, 1023200, 3053800, 3117800, 4052200, 2038000, 4047600, 3027700, 3083000, 3019000, 3057400, 4059900, 3035900, 1004800, 2037504, 4023800, 3061002, 3026700, 2032400, 3018000, 3082000, 1012500, 1025300, 4046100, 4007700, 3051800, 3115800, 4050201, 4003100, 3030300, 4156700, 2014500, 4040700, 2022702, 2022703, 4049200, 4062000, 3102000, 2018102, 3084600, 2035000, 3037500, 3041600, 3092800, 3054400, 1006400, 3118400, 1019200, 2004800, 1001800, 2017100, 4077903, 2021200, 4018000, 3036502, 3091800, 3019100, 2033500, 3023200, 2003300, 4000100, 4055400, 2020200, 4033900, 4091900, 3065200, 3116400, 2019700, 4029300, 3030900, 5027704, 4059000, 4037500, 3039100, 4011900, 1020800, 2023301, 2023302, 1003400, 3102600, 4028300, 4045200, 3085200, 3059600, 1002900, 4049301, 3055000, 4027800, 4079000, 3012600, 3101600, 3076000, 1006500, 3020200, 1010601, 1010602, 1014700, 4009900, 3049400, 1018300, 1031100, 4026300, 2029500, 4056000, 3096000, 3023300, 2012102, 3048400, 3074000, 3039700, 1025500, 4046300, 3116000, 4012000, 1016300, 3055600, 4028400, 3123700, 2022901, 1011700, 3008500, 4011000, 4117500, 2018301])]\n",
    "    df= get_geo(df)\n",
    "    df = clustering(df)\n",
    "    df = encoding(df)\n",
    "    #df = tree(df)\n",
    "    df = date(df)\n",
    "    df = category(df)\n",
    "    df = minmax(df)\n",
    "    return df\n",
    "\n",
    "def get_test(df):\n",
    "    df.fillna('NULL',inplace=True)\n",
    "    df= get_geo(df)\n",
    "    df = clustering(df)\n",
    "    df = encoding(df)\n",
    "    #df = tree(df)\n",
    "    df = date(df)\n",
    "    df = category(df)\n",
    "    df = minmax(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d2687-9501-4125-86af-2c356ebfde60",
   "metadata": {},
   "source": [
    "## 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc3af64-05eb-4f34-94a5-cafaef7d4061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "経過時間： 6.403079032897949s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = get_train(df)\n",
    "test = get_test(test)\n",
    "end = time.time()\n",
    "print(f'経過時間： {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ae13b0-112a-467b-9757-69b1a6ef99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['health']\n",
    "X = df.drop('health',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcd0e59-7c52-4f92-bb80-b065bd14cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['spc_common','nta','zip_city','problems','spc_latin','nta_name']\n",
    "for col in target_columns:\n",
    "    X,test = target_enc(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05beef6-39f2-47b0-b697-3718b235ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_dbh             float64\n",
       "borocode               int64\n",
       "boro_ct                int64\n",
       "cb_num                 int64\n",
       "st_senate              int64\n",
       "st_assem               int64\n",
       "cncldist               int64\n",
       "latitude             float64\n",
       "longitude            float64\n",
       "cluster                int32\n",
       "label_curb_loc         int64\n",
       "label_steward          int64\n",
       "label_guards           int64\n",
       "label_sidewalk         int64\n",
       "label_boroname         int64\n",
       "label_user_type        int64\n",
       "created_year           int32\n",
       "created_month          int32\n",
       "created_date           int32\n",
       "created_weekdat        int32\n",
       "target_spc_common    float64\n",
       "target_nta           float64\n",
       "target_zip_city      float64\n",
       "target_problems      float64\n",
       "target_spc_latin     float64\n",
       "target_nta_name      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc5470-86ff-4417-be1a-46de1c0b573c",
   "metadata": {},
   "source": [
    "## drop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c88da4-982f-4ef3-8f16-d45fcfca84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(df):\n",
    "    drop_columns = ['label_boroname','borocode','target_spc_latin','target_nta_name']\n",
    "    df.drop(drop_columns,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a9e1d7-9baa-43f4-bb2b-88d365d743c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = drop(X)\n",
    "test = drop(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740be744-aaae-4cc0-aac8-fd5c1b89943c",
   "metadata": {},
   "source": [
    "## モデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e00f8a-6446-404f-aea7-4499be3a7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化 (Standardization)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\"\"\"\n",
    "scaler_standard = StandardScaler()\n",
    "data_standardized = scaler_standard.fit_transform(X)\n",
    "data_standardized = scaler_standard.fit_transform(test)\n",
    "\"\"\"\n",
    "# 正規化 (Normalization)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_normalized = scaler_minmax.fit_transform(X)\n",
    "data_normalized = scaler_minmax.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f0a466-4ee0-41e7-b9ac-b82184d3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights = dict(zip(np.unique(y), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5058bb-f146-40a8-944d-37060900abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights[0] = 1.9503831417624522\n",
    "class_weights[1] = 0.4366529421856236\n",
    "class_weights[2] = 7.101843547583457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "853d4f94-98e7-4955-af2c-7a3583b790d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.9503831417624522, 1: 0.4366529421856236, 2: 7.101843547583457}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0e78b9b-2d5e-48e3-9e92-2218fd449d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータセットのラベルのカウント: {0: 3481, 1: 15545, 2: 669}\n",
      "リサンプリング後のラベルのカウント: {0: 1740, 1: 7772, 2: 669}\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "sampling_strategy = {1: int(15545*0.5), 0: int(3481*0.5),2:669} # 0:多数派クラス, 1:少数派クラス\n",
    "\n",
    "# アンダーサンプリングの設定と実行\n",
    "under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=28)\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X, y)\n",
    "\n",
    "# 結果の表示\n",
    "print(\"元のデータセットのラベルのカウント:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "print(\"リサンプリング後のラベルのカウント:\", dict(zip(*np.unique(y_resampled, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc2513b1-be7a-4ccb-a4ec-e2fbdde9c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              feature    importance\n",
      "0            tree_dbh   8736.716187\n",
      "1             boro_ct   8162.519876\n",
      "2              cb_num   2964.117211\n",
      "3           st_senate   2734.054064\n",
      "4            st_assem   4761.254468\n",
      "5            cncldist   3238.192980\n",
      "6            latitude   1546.599710\n",
      "7           longitude   1803.674899\n",
      "8             cluster    289.356551\n",
      "9      label_curb_loc    420.292856\n",
      "10      label_steward   2205.563905\n",
      "11       label_guards   1303.034530\n",
      "12     label_sidewalk   1263.090472\n",
      "13    label_user_type   1774.190362\n",
      "14       created_year    893.979351\n",
      "15      created_month   5078.418664\n",
      "16       created_date   9666.762337\n",
      "17    created_weekdat   5209.712086\n",
      "18  target_spc_common  13108.693729\n",
      "19         target_nta  11901.313753\n",
      "20    target_zip_city   6871.624630\n",
      "21    target_problems   8441.655358\n",
      "Mean F1 Score (Macro F1 Score): 0.34090609635589614\n",
      "              feature    importance\n",
      "0            tree_dbh   8818.287774\n",
      "1             boro_ct   8991.119088\n",
      "2              cb_num   2925.024228\n",
      "3           st_senate   2648.209401\n",
      "4            st_assem   5187.854511\n",
      "5            cncldist   3185.077302\n",
      "6            latitude   1384.452589\n",
      "7           longitude   1840.497352\n",
      "8             cluster    168.306002\n",
      "9      label_curb_loc    429.576463\n",
      "10      label_steward   2162.390765\n",
      "11       label_guards   1413.119531\n",
      "12     label_sidewalk   1420.543773\n",
      "13    label_user_type   2185.975188\n",
      "14       created_year    915.743768\n",
      "15      created_month   4914.676208\n",
      "16       created_date   9892.664023\n",
      "17    created_weekdat   4713.353510\n",
      "18  target_spc_common  12427.808340\n",
      "19         target_nta  11879.806220\n",
      "20    target_zip_city   6310.743885\n",
      "21    target_problems   8451.310991\n",
      "Mean F1 Score (Macro F1 Score): 0.3274826466128904\n",
      "              feature    importance\n",
      "0            tree_dbh   8267.214320\n",
      "1             boro_ct   8123.826571\n",
      "2              cb_num   3367.798959\n",
      "3           st_senate   2677.224549\n",
      "4            st_assem   4785.425391\n",
      "5            cncldist   2893.255158\n",
      "6            latitude   1635.739583\n",
      "7           longitude   1793.926595\n",
      "8             cluster    188.591991\n",
      "9      label_curb_loc    453.106971\n",
      "10      label_steward   2151.190461\n",
      "11       label_guards   1500.299277\n",
      "12     label_sidewalk   1360.791549\n",
      "13    label_user_type   2179.606394\n",
      "14       created_year    847.704359\n",
      "15      created_month   4503.911794\n",
      "16       created_date   9287.575987\n",
      "17    created_weekdat   5008.585500\n",
      "18  target_spc_common  13085.687154\n",
      "19         target_nta  12269.170979\n",
      "20    target_zip_city   6716.836201\n",
      "21    target_problems   9129.791261\n",
      "Mean F1 Score (Macro F1 Score): 0.3291604208036482\n",
      "              feature    importance\n",
      "0            tree_dbh   8465.504637\n",
      "1             boro_ct   8436.042510\n",
      "2              cb_num   3018.127345\n",
      "3           st_senate   2802.466234\n",
      "4            st_assem   4833.481829\n",
      "5            cncldist   3285.910764\n",
      "6            latitude   1667.853784\n",
      "7           longitude   1461.994603\n",
      "8             cluster    176.440373\n",
      "9      label_curb_loc    382.271323\n",
      "10      label_steward   2252.857285\n",
      "11       label_guards   1446.717431\n",
      "12     label_sidewalk   1527.171526\n",
      "13    label_user_type   1932.434600\n",
      "14       created_year    954.607644\n",
      "15      created_month   5125.386402\n",
      "16       created_date   9144.041535\n",
      "17    created_weekdat   4658.591905\n",
      "18  target_spc_common  13291.651582\n",
      "19         target_nta  11969.111441\n",
      "20    target_zip_city   6821.499083\n",
      "21    target_problems   8784.457428\n",
      "Mean F1 Score (Macro F1 Score): 0.3364604224978272\n",
      "              feature    importance\n",
      "0            tree_dbh   8715.742637\n",
      "1             boro_ct   8509.909233\n",
      "2              cb_num   3193.292633\n",
      "3           st_senate   2548.865071\n",
      "4            st_assem   4943.438770\n",
      "5            cncldist   3358.491106\n",
      "6            latitude   1712.598864\n",
      "7           longitude   1822.847738\n",
      "8             cluster    237.674702\n",
      "9      label_curb_loc    503.780780\n",
      "10      label_steward   2123.423686\n",
      "11       label_guards   1577.991061\n",
      "12     label_sidewalk   1364.667970\n",
      "13    label_user_type   2025.778912\n",
      "14       created_year    913.094769\n",
      "15      created_month   4557.757065\n",
      "16       created_date   9330.402014\n",
      "17    created_weekdat   4657.196314\n",
      "18  target_spc_common  12853.392925\n",
      "19         target_nta  12392.458086\n",
      "20    target_zip_city   6901.352224\n",
      "21    target_problems   8184.382394\n",
      "Mean F1 Score (Macro F1 Score): 0.33740307720201274\n",
      " \n",
      "average_f1:0.33428253269445496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import statistics\n",
    "from scipy.stats import mode\n",
    "\n",
    "average_f1 = []\n",
    "predictions = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_idx, val_idx in skf.split(X_resampled,y_resampled):\n",
    "    X_train, X_val = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n",
    "    y_train, y_val = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        'class_weight': class_weights,\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    mean_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    average_f1.append(mean_f1)\n",
    "    feature_gains = model.booster_.feature_importance(importance_type='gain')\n",
    "    feature = pd.DataFrame({'feature': X.columns, 'importance': feature_gains})\n",
    "    feature.sort_values(by='importance',ascending=False)\n",
    "    print(feature)\n",
    "    print(\"Mean F1 Score (Macro F1 Score):\", mean_f1)\n",
    "    \n",
    "    predictions.append(model.predict(test))\n",
    "\n",
    "average_f1 = statistics.mean(average_f1)\n",
    "print(' ')\n",
    "print(f'average_f1:{average_f1}')\n",
    "\n",
    "weighted_votes = np.zeros((len(test), len(class_weights)))\n",
    "\n",
    "for prediction in predictions:\n",
    "    for idx, pred in enumerate(prediction):\n",
    "        weighted_votes[idx, pred] += class_weights[pred]\n",
    "\n",
    "weighted_majority_vote = np.argmax(weighted_votes, axis=1)\n",
    "\n",
    "skf_submission = pd.DataFrame({'Id': Id['Unnamed: 0'], 'predict': weighted_majority_vote})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a4e6d0d-628d-43f7-9f54-10732a1a984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf_submission.to_csv('submission_1-2_2122.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce56b51-e75a-4108-be00-1c8e4b724a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict\n",
       "1    15206\n",
       "0     3839\n",
       "2      657\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf_submission['predict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7a4e5b8-b200-4c28-a57b-3c369975763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.0, Best macro F1 Score: 0.3237089067572702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# LightGBMモデルを訓練し、検証データセット上での予測確率を取得\n",
    "# model = 訓練済みのLightGBMモデル\n",
    "# X_val = 検証データセットの特徴量\n",
    "predicted_probabilities = model.predict(X_val)\n",
    "\n",
    "# 閾値を変えながらF1スコアを計算\n",
    "best_threshold = 0\n",
    "best_score = 0\n",
    "\n",
    "for threshold in np.linspace(0, 1, 100):\n",
    "    # 閾値を用いて確率をクラスラベルに変換\n",
    "    predicted_labels = (predicted_probabilities > threshold).astype(int)\n",
    "\n",
    "    # macro F1スコアを計算\n",
    "    score = f1_score(y_val, predicted_labels, average='macro')\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}, Best macro F1 Score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d6d6949-6479-41c1-8905-60e971cd0bf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
